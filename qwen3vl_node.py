import torch
import time
import json
import random
import platform
import psutil
import numpy as np

from packaging import version
from PIL import Image
from enum import Enum
from pathlib import Path
import transformers
from transformers import AutoModelForImageTextToText, AutoProcessor, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download as hf_snapshot_download
import folder_paths
import gc

# å°è¯•å¯¼å…¥ ModelScopeï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ HuggingFace
try:
    from modelscope.hub.snapshot_download import snapshot_download as ms_snapshot_download
    MODELSCOPE_AVAILABLE = True
except ImportError:
    ms_snapshot_download = None
    MODELSCOPE_AVAILABLE = False
    print("[Qwen3VL] [è­¦å‘Š] ModelScope æœªå®‰è£…ï¼ŒModelScope æ¨¡å‹å°†æ— æ³•ä¸‹è½½ã€‚è¯·è¿è¡Œ: pip install modelscope")

NODE_DIR = Path(__file__).parent
CONFIG_PATH = NODE_DIR / "config.json"
MODEL_CONFIGS = {}
SYSTEM_PROMPTS = {}

def load_model_configs():
    """åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶"""
    global MODEL_CONFIGS, SYSTEM_PROMPTS
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            MODEL_CONFIGS = json.load(f)
            SYSTEM_PROMPTS = MODEL_CONFIGS.get("_system_prompts", {})
    except FileNotFoundError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æœªæ‰¾åˆ° {CONFIG_PATH}")
        MODEL_CONFIGS, SYSTEM_PROMPTS = {}, {}
    except json.JSONDecodeError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶è§£æå¤±è´¥")
        MODEL_CONFIGS, SYSTEM_PROMPTS = {}, {}

    # åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹é…ç½®
    custom_path = NODE_DIR / "custom_models.json"
    if custom_path.exists():
        try:
            with open(custom_path, "r", encoding="utf-8") as f:
                custom_data = json.load(f) or {}

            user_models = custom_data.get("hf_models", {}) or custom_data.get("models", {})

            if user_models:
                MODEL_CONFIGS.update(user_models)
                print(f"[Qwen3VL] [æˆåŠŸ] å·²åŠ è½½ {len(user_models)} ä¸ªè‡ªå®šä¹‰æ¨¡å‹")
            else:
                print("[Qwen3VL] [è­¦å‘Š] æ‰¾åˆ° custom_models.json ä½†æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹æ¡ç›®")
        except Exception as e:
            print(f"[Qwen3VL] [è­¦å‘Š] åŠ è½½ custom_models.json å¤±è´¥ â†’ {e}")
    else:
        print("[Qwen3VL] [ä¿¡æ¯] æœªæ‰¾åˆ° custom_models.jsonï¼Œè·³è¿‡è‡ªå®šä¹‰æ¨¡å‹")

    # æ˜¾ç¤ºæ¨¡å‹åŠ è½½ä¿¡æ¯
    print("====  Qwen3 VL æœ¬åœ°æ¨¡å‹å¯¹è¯/æ‰“æ ‡èŠ‚ç‚¹ - NakuNode ====")

    # æ£€æŸ¥æ˜¯å¦æœ‰ç ´é™æ¨¡å‹
    ablated_models = []
    for model_name, model_info in MODEL_CONFIGS.items():
        if not model_name.startswith('_') and model_info.get('abliterated'):
            ablated_models.append(model_name)

    if ablated_models:
        print("æ³¨æ„ï¼š")
        for model in ablated_models:
            print(f"{model} ï¼ˆç ´é™æ¨¡å‹ï¼‰")

    print("")
    print("==== æ¨¡å‹åˆ†ç±» ====")
    print("Instructï¼šæŒ‡ä»¤æ¨¡å¼")
    print("Thinkingï¼šæ¨ç†æ¨¡å¼")
    print("")
    print("==== æ¨¡å‹ä½¿ç”¨å»ºè®® ====")
    print("Qwen3-VL-4B-Instructï¼šå»ºè®® 16G åŠä»¥ä¸‹æ˜¾å­˜ä½¿ç”¨ã€‚")
    print("Qwen3-VL-8B-Instructï¼šå»ºè®® 24G åŠä»¥ä¸‹æ˜¾å­˜ä½¿ç”¨ã€‚")

if not MODEL_CONFIGS:
    load_model_configs()

class Quantization(str, Enum):
    """é‡åŒ–é€‰é¡¹æšä¸¾"""
    Q4_BIT = "4-bit (èŠ‚çœæ˜¾å­˜)"
    Q8_BIT = "8-bit (å¹³è¡¡)"
    NONE = "None (FP16)"
    
    @classmethod
    def get_values(cls):
        return [item.value for item in cls]

def get_model_info(model_name: str) -> dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return MODEL_CONFIGS.get(model_name, {})

def get_device_info() -> dict:
    """è·å–è®¾å¤‡ä¿¡æ¯"""
    gpu_info = {}
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        total_mem = props.total_memory / 1024**3
        gpu_info = {
            "available": True,
            "total_memory": total_mem,
            "free_memory": total_mem - (torch.cuda.memory_allocated(0) / 1024**3)
        }
    else:
        gpu_info = {"available": False, "total_memory": 0, "free_memory": 0}

    sys_mem = psutil.virtual_memory()
    sys_mem_info = {
        "total": sys_mem.total / 1024**3,
        "available": sys_mem.available / 1024**3
    }

    device_info = {
        "gpu": gpu_info,
        "system_memory": sys_mem_info,
        "device_type": "cpu",
        "recommended_device": "cpu",
        "memory_sufficient": True,
        "warning_message": ""
    }

    if platform.system() == "Darwin" and platform.processor() == "arm":
        device_info.update({
            "device_type": "apple_silicon",
            "recommended_device": "mps"
        })
        if sys_mem_info["total"] < 16:
            device_info.update({
                "memory_sufficient": False,
                "warning_message": "Apple Silicon å†…å­˜å°äº 16GBï¼Œæ€§èƒ½å¯èƒ½å—å½±å“"
            })
    elif gpu_info["available"]:
        device_info.update({
            "device_type": "nvidia_gpu",
            "recommended_device": "cuda"
        })
        if gpu_info["total_memory"] < 8:
            device_info.update({
                "memory_sufficient": False,
                "warning_message": "GPU æ˜¾å­˜å°äº 8GBï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™"
            })
    
    return device_info

def check_memory_requirements(model_name: str, quantization: str, device_info: dict) -> str:
    """æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶è‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ«"""
    model_info = get_model_info(model_name)
    vram_req = model_info.get("vram_requirement", {})
    
    quant_map = {
        Quantization.Q4_BIT: vram_req.get("4bit", 0),
        Quantization.Q8_BIT: vram_req.get("8bit", 0),
        Quantization.NONE: vram_req.get("full", 0)
    }
    
    base_memory = quant_map.get(quantization, 0)
    device = device_info["recommended_device"]
    use_cpu_mps = device in ["cpu", "mps"]
    
    required_mem = base_memory * (1.5 if use_cpu_mps else 1.0)
    available_mem = device_info["system_memory"]["available"] if use_cpu_mps else device_info["gpu"]["free_memory"]
    mem_type = "ç³»ç»Ÿå†…å­˜" if use_cpu_mps else "GPUæ˜¾å­˜"

    if required_mem * 1.2 > available_mem:
        print(f"è­¦å‘Š: {mem_type} ä¸è¶³ ({available_mem:.2f}GB å¯ç”¨)ã€‚é™ä½é‡åŒ–çº§åˆ«...")
        if quantization == Quantization.NONE:
            return Quantization.Q8_BIT
        if quantization == Quantization.Q8_BIT:
            return Quantization.Q4_BIT
        raise RuntimeError(f"{mem_type} ä¸è¶³ï¼Œå³ä½¿ä½¿ç”¨ 4-bit é‡åŒ–ä¹Ÿæ— æ³•è¿è¡Œ")
    
    return quantization

def check_flash_attention() -> bool:
    """æ£€æŸ¥æ˜¯å¦æ”¯æŒ Flash Attention 2"""
    try:
        import flash_attn
        if torch.cuda.is_available():
            major, _ = torch.cuda.get_device_capability()
            return major >= 8
    except ImportError:
        return False
    return False

class ImageProcessor:
    """å›¾åƒå¤„ç†å™¨"""
    def to_pil(self, image_tensor: torch.Tensor) -> Image.Image:
        """å°† ComfyUI å›¾åƒå¼ é‡è½¬æ¢ä¸º PIL Image"""
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
        image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        return Image.fromarray(image_np)

class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨
    
    æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼šComfyUI/models/prompt_generator/
    """
    def __init__(self, configs):
        self.configs = configs
        # ä¿®æ”¹æ¨¡å‹å­˜å‚¨è·¯å¾„ä¸º prompt_generator æ–‡ä»¶å¤¹
        self.models_dir = Path(folder_paths.models_dir) / "prompt_generator"
        self.models_dir.mkdir(parents=True, exist_ok=True)

    def ensure_model_available(self, model_name):
        """ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½
        
        æ¨¡å‹ä¼šç›´æ¥ä¸‹è½½åˆ° ComfyUI/models/prompt_generator/ ç›®å½•
        å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸ä¼šé‡å¤ä¸‹è½½
        æ”¯æŒ HuggingFace å’Œ ModelScope ä¸¤ç§æ¥æº
        """
        model_info = self.configs.get(model_name)
        if not model_info:
            raise ValueError(f"æ¨¡å‹ '{model_name}' æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°")

        repo_id = model_info['repo_id']
        source = model_info.get('source', 'huggingface')  # é»˜è®¤ä½¿ç”¨ HuggingFace
        model_folder_name = repo_id.split('/')[-1]
        model_path = self.models_dir / model_folder_name
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å®Œæ•´ä¸‹è½½ï¼ˆæ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
        config_file = model_path / "config.json"
        model_file = model_path / "model.safetensors"
        # æœ‰äº›æ¨¡å‹ä½¿ç”¨åˆ†ç‰‡å­˜å‚¨
        model_index = model_path / "model.safetensors.index.json"
        
        if model_path.exists() and config_file.exists():
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå®Œæ•´æ¨¡å‹æˆ–åˆ†ç‰‡æ¨¡å‹ï¼‰
            if model_file.exists() or model_index.exists():
                print(f"[æˆåŠŸ] æ¨¡å‹ '{model_name}' å·²å­˜åœ¨äº {model_path}")
                print(f"[è·¯å¾„] æ¨¡å‹è·¯å¾„: {model_path}")
                return str(model_path)
            else:
                print(f"[è­¦å‘Š] æ¨¡å‹ç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å®Œæ•´ï¼Œå°†é‡æ–°ä¸‹è½½...")
        
        # æ£€æŸ¥ ModelScope æ¨¡å‹æ˜¯å¦éœ€è¦å®‰è£…ä¾èµ–
        if source == 'modelscope' and not MODELSCOPE_AVAILABLE:
            raise RuntimeError(
                f"æ¨¡å‹ '{model_name}' æ¥è‡ª ModelScopeï¼Œä½† ModelScope åº“æœªå®‰è£…ã€‚\n"
                f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                f"pip install modelscope\n"
                f"æˆ–è€…ä½¿ç”¨ HuggingFace é•œåƒç«™æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {model_path}"
            )
        
        print(f"ğŸ“¥ æ­£åœ¨ä» {source.upper()} ä¸‹è½½æ¨¡å‹ '{model_name}' åˆ° {model_path}...")
        print(f"[è·¯å¾„] ç›®æ ‡è·¯å¾„: {model_path}")
        print("â³ æç¤ºï¼šé¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        model_path.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®æ¥æºé€‰æ‹©ä¸‹è½½å‡½æ•°
        if source == 'modelscope':
            snapshot_download_func = ms_snapshot_download
            download_kwargs = {
                "model_id": repo_id,
                "cache_dir": str(model_path.parent),
                "local_dir": str(model_path),
            }
            source_url = f"https://modelscope.cn/models/{repo_id}"
        else:
            snapshot_download_func = hf_snapshot_download
            download_kwargs = {
                "repo_id": repo_id,
                "local_dir": str(model_path),
                "local_dir_use_symlinks": False,
                "ignore_patterns": ["*.md", "*.txt", ".gitattributes"],
                "resume_download": True,
                "max_workers": 4
            }
            source_url = f"https://huggingface.co/{repo_id}"
        
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜
        max_retries = 3
        for attempt in range(max_retries):
            try:
                downloaded_path = snapshot_download_func(**download_kwargs)
                print(f"[æˆåŠŸ] æ¨¡å‹ '{model_name}' ä¸‹è½½å®Œæˆï¼")
                print(f"[è·¯å¾„] æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
                return str(model_path)
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"[è­¦å‘Š] ä¸‹è½½å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {str(e)}")
                    print(f"â³ ç­‰å¾… 5 ç§’åé‡è¯•...")
                    time.sleep(5)
                else:
                    print(f"[å¤±è´¥] ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    error_msg = f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}\nå»ºè®®ï¼š\n"
                    if source == 'modelscope':
                        error_msg += (
                            f"1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                            f"2. ç¡®ä¿å·²å®‰è£… ModelScope: pip install modelscope\n"
                            f"3. æ‰‹åŠ¨ä» {source_url} ä¸‹è½½æ¨¡å‹åˆ°: {model_path}\n"
                        )
                    else:
                        error_msg += (
                            f"1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                            f"2. è®¾ç½® HF_ENDPOINT ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒæºï¼ˆå¦‚ï¼šhttps://hf-mirror.comï¼‰\n"
                            f"3. æ‰‹åŠ¨ä» {source_url} ä¸‹è½½æ¨¡å‹åˆ°: {model_path}\n"
                        )
                    raise RuntimeError(error_msg)

class Qwen3VL_Advanced:
    """Qwen3-VL é«˜çº§èŠ‚ç‚¹ - æ”¯æŒå›¾åƒå’Œè§†é¢‘ç†è§£"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model_name = None
        self.current_quantization = None
        self.current_device = None
        self.last_seed = -1
        self.device_info = get_device_info()
        self.downloader = ModelDownloader(MODEL_CONFIGS)
        self.image_processor = ImageProcessor()
        print(f"Qwen3VL èŠ‚ç‚¹å·²åˆå§‹åŒ–ã€‚è®¾å¤‡: {self.device_info['device_type']}")
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}")

    def clear_model_resources(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.model is not None:
            print("é‡Šæ”¾æ¨¡å‹èµ„æº...")
            del self.model, self.processor, self.tokenizer
            self.model = self.processor = self.tokenizer = None
            self.current_model_name = self.current_quantization = self.current_device = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def load_model(self, model_name: str, quantization_str: str, device: str = "auto"):
        """åŠ è½½æ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
            quantization_str: é‡åŒ–çº§åˆ«å­—ç¬¦ä¸²
            device: è®¾å¤‡ç±»å‹ (auto/cuda/cpu/mps)

        Raises:
            ValueError: å½“ GPU ä¸æ”¯æŒ FP8 æ¨¡å‹æˆ–ä½¿ç”¨ abliterated æ¨¡å‹æ—¶
        """
        effective_device = self.device_info["recommended_device"] if device == "auto" else device

        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”é…ç½®ç›¸åŒï¼Œåˆ™è·³è¿‡
        if (self.model is not None and
            self.current_model_name == model_name and
            self.current_quantization == quantization_str and
            self.current_device == effective_device):
            return

        self.clear_model_resources()

        model_info = get_model_info(model_name)

        # æ£€æŸ¥ abliterated æ¨¡å‹çš„è­¦å‘Š
        if model_info.get("abliterated"):
            warning_msg = model_info.get("warning", "æ­¤æ¨¡å‹å·²ç§»é™¤å®‰å…¨è¿‡æ»¤")
            print(f"\n[è­¦å‘Š] è­¦å‘Š: {warning_msg}\n")
        
        # æ£€æŸ¥ FP8 é‡åŒ–æ¨¡å‹çš„ GPU è®¡ç®—èƒ½åŠ›è¦æ±‚
        if model_info.get("quantized"):
            if self.device_info["gpu"]["available"]:
                major, minor = torch.cuda.get_device_capability()
                cc = major + minor / 10
                if cc < 8.9:
                    raise ValueError(
                        f"FP8 æ¨¡å‹éœ€è¦è®¡ç®—èƒ½åŠ› 8.9 æˆ–æ›´é«˜çš„ GPU (ä¾‹å¦‚ RTX 4090)ã€‚"
                        f"æ‚¨çš„ GPU è®¡ç®—èƒ½åŠ›ä¸º {cc}ã€‚è¯·é€‰æ‹©é FP8 æ¨¡å‹ã€‚"
                    )

        model_path = self.downloader.ensure_model_available(model_name)
        adjusted_quantization = check_memory_requirements(model_name, quantization_str, self.device_info)
        
        quant_config, load_dtype = None, torch.float16
        
        # ä»…å¯¹éé¢„é‡åŒ–æ¨¡å‹åº”ç”¨é‡åŒ–é…ç½®
        if not get_model_info(model_name).get("quantized", False):
            if adjusted_quantization == Quantization.Q4_BIT:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
                load_dtype = None
            elif adjusted_quantization == Quantization.Q8_BIT:
                quant_config = BitsAndBytesConfig(load_in_8bit=True)
                load_dtype = None

        device_map = "auto"
        if effective_device == "cuda" and torch.cuda.is_available():
            device_map = {"": 0}

        # æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,
            "attn_implementation": "flash_attention_2" if check_flash_attention() else "sdpa",
            "use_safetensors": True,
            "trust_remote_code": True  # abliterated æ¨¡å‹éœ€è¦
        }
        
        if quant_config:
            load_kwargs["quantization_config"] = quant_config

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{model_name}'...")
        # åŠ è½½æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
        self.model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs).eval()
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        self.current_model_name = model_name
        self.current_quantization = quantization_str
        self.current_device = effective_device
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")

    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        model_names = [name for name in MODEL_CONFIGS.keys() if not name.startswith('_')]
        default_model = model_names[4] if len(model_names) > 4 else model_names[0]
        preset_prompts = MODEL_CONFIGS.get("_preset_prompts", ["è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡"])

        return {
            "required": {
                "æ¨¡å‹é€‰æ‹©": (model_names, {"default": default_model}),
                "é‡åŒ–çº§åˆ«": (list(Quantization.get_values()), {"default": Quantization.NONE}),
                "é¢„è®¾æç¤ºè¯": (preset_prompts, {"default": preset_prompts[2]}),
                "è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "å¯é€‰æ‹©é¢„è®¾æç¤ºè¯æˆ–è¾“å…¥è‡ªå®šä¹‰æç¤ºè¯"
                }),
                "æœ€å¤§ä»¤ç‰Œæ•°": ("INT", {"default": 1024, "min": 64, "max": 4096, "step": 16}),
                "é‡‡æ ·æ¸©åº¦": ("FLOAT", {"default": 0.6, "min": 0.1, "max": 1.0, "step": 0.1}),
                "æ ¸é‡‡æ ·å‚æ•°": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "æŸæœç´¢æ•°é‡": ("INT", {"default": 1, "min": 1, "max": 10, "step": 1}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.2, "min": 0.0, "max": 2.0, "step": 0.01}),
                "è§†é¢‘å¸§æ•°": ("INT", {"default": 16, "min": 1, "max": 64, "step": 1}),
                "è®¾å¤‡é€‰æ‹©": (["auto", "cuda", "cpu", "mps"], {"default": "auto"}),
                "å¼€å¯TF32åŠ é€Ÿ": ("BOOLEAN", {"default": False, "tooltip": "å¯ç”¨TF32åŠ é€Ÿï¼ˆä»…æ”¯æŒAmpereåŠä»¥ä¸Šæ¶æ„æ˜¾å¡ï¼Œå¦‚30/40/50ç³»ï¼Œèƒ½æ˜¾è‘—æå‡é€Ÿåº¦ï¼‰"}),
                "ä¿æŒæ¨¡å‹åŠ è½½": ("BOOLEAN", {"default": True}),
                "éšæœºç§å­": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"
                }),
                "ç§å­æ§åˆ¶": (["éšæœº", "å›ºå®š", "é€’å¢"], {"default": "éšæœº"}),
            },
            "optional": {
                "å›¾åƒ1": ("IMAGE",),
                "å›¾åƒ2": ("IMAGE",),
                "å›¾åƒ3": ("IMAGE",),
                "å›¾åƒ4": ("IMAGE",),
                "è§†é¢‘": ("IMAGE",),
                "Qwen3VLé¢å¤–é€‰é¡¹": ("QWEN3VL_EXTRA_OPTIONS", {
                    "tooltip": "å¯é€‰çš„Qwen3VLé¢å¤–é€‰é¡¹ï¼Œè¿æ¥Qwen3VLé¢å¤–é€‰é¡¹èŠ‚ç‚¹"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æ–‡æœ¬è¾“å‡º",)
    FUNCTION = "process"
    CATEGORY = "NakuNode-QWen3VL"

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        seed_control = kwargs.get("ç§å­æ§åˆ¶", "éšæœº")
        seed = kwargs.get("éšæœºç§å­", -1)
        
        # éšæœºå’Œé€’å¢æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶æ›´æ–° (è¿”å› NaN)
        if seed_control in ["éšæœº", "é€’å¢"]:
            return float("nan")
        
        # å›ºå®šæ¨¡å¼ä¸‹ï¼Œä»…å½“ç§å­å€¼å˜åŒ–æ—¶æ›´æ–°
        return seed

    @torch.no_grad()
    def process(self, **kwargs):
        """å¤„ç†å›¾åƒæˆ–è§†é¢‘è¾“å…¥å¹¶ç”Ÿæˆæ–‡æœ¬ - ä½¿ç”¨kwargså¤„ç†å‚æ•°å"""
        # æå–å‚æ•°
        æ¨¡å‹åç§° = kwargs.get("æ¨¡å‹é€‰æ‹©")
        é‡åŒ–çº§åˆ« = kwargs.get("é‡åŒ–çº§åˆ«")
        é¢„è®¾æç¤ºè¯ = kwargs.get("é¢„è®¾æç¤ºè¯")
        æœ€å¤§ä»¤ç‰Œæ•° = kwargs.get("æœ€å¤§ä»¤ç‰Œæ•°")
        é‡‡æ ·æ¸©åº¦ = kwargs.get("é‡‡æ ·æ¸©åº¦")
        æ ¸é‡‡æ ·å‚æ•° = kwargs.get("æ ¸é‡‡æ ·å‚æ•°")
        é‡å¤æƒ©ç½š = kwargs.get("é‡å¤æƒ©ç½š")
        æŸæœç´¢æ•°é‡ = kwargs.get("æŸæœç´¢æ•°é‡")
        è§†é¢‘å¸§æ•° = kwargs.get("è§†é¢‘å¸§æ•°")
        è®¾å¤‡é€‰æ‹© = kwargs.get("è®¾å¤‡é€‰æ‹©")
        éšæœºç§å­ = kwargs.get("éšæœºç§å­")
        è‡ªå®šä¹‰æç¤ºè¯ = kwargs.get("è‡ªå®šä¹‰æç¤ºè¯", "")
        å›¾åƒ1 = kwargs.get("å›¾åƒ1")
        å›¾åƒ2 = kwargs.get("å›¾åƒ2")
        å›¾åƒ3 = kwargs.get("å›¾åƒ3")
        å›¾åƒ4 = kwargs.get("å›¾åƒ4")
        è§†é¢‘ = kwargs.get("è§†é¢‘")
        ä¿æŒæ¨¡å‹åŠ è½½ = kwargs.get("ä¿æŒæ¨¡å‹åŠ è½½", True)
        å¼€å¯TF32åŠ é€Ÿ = kwargs.get("å¼€å¯TF32åŠ é€Ÿ", False)
        ç§å­æ§åˆ¶ = kwargs.get("ç§å­æ§åˆ¶", "éšæœº")
        extra_options = kwargs.get("Qwen3VLé¢å¤–é€‰é¡¹", None)
        start_time = time.time()

        # è®¾ç½® TF32 åŠ é€Ÿ
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = å¼€å¯TF32åŠ é€Ÿ
            torch.backends.cudnn.allow_tf32 = å¼€å¯TF32åŠ é€Ÿ
            if å¼€å¯TF32åŠ é€Ÿ:
                print("[åŠ é€Ÿ] å·²å¼€å¯ TF32 åŠ é€Ÿæ¨¡å¼")
        
        # ç§å­é€»è¾‘å¤„ç†
        if ç§å­æ§åˆ¶ == "å›ºå®š":
            effective_seed = éšæœºç§å­ if éšæœºç§å­ != -1 else random.randint(0, 2147483647)
        elif ç§å­æ§åˆ¶ == "éšæœº":
            effective_seed = random.randint(0, 2147483647)
        elif ç§å­æ§åˆ¶ == "é€’å¢":
            if self.last_seed == -1:
                effective_seed = éšæœºç§å­ if éšæœºç§å­ != -1 else random.randint(0, 2147483647)
            else:
                effective_seed = self.last_seed + 1
        else:
            effective_seed = random.randint(0, 2147483647)
        
        self.last_seed = effective_seed
        print(f"ä½¿ç”¨éšæœºç§å­: {effective_seed} (æ¨¡å¼: {ç§å­æ§åˆ¶})")
        torch.manual_seed(effective_seed)
        
        # æ£€æŸ¥ transformers ç‰ˆæœ¬
        if version.parse(transformers.__version__) < version.parse("4.57.0"):
            raise RuntimeError(f"transformers ç‰ˆæœ¬è¿‡ä½: å½“å‰ç‰ˆæœ¬ {transformers.__version__}, éœ€è¦ >= 4.57.0")

        load_start = time.time()
        self.load_model(æ¨¡å‹åç§°, é‡åŒ–çº§åˆ«, è®¾å¤‡é€‰æ‹©)
        load_time = time.time() - load_start
        effective_device = self.current_device
        
        # ç¡®å®šä½¿ç”¨çš„æç¤ºè¯ï¼ˆå›¾åƒ/è§†é¢‘åæ¨ä¸“ç”¨ï¼‰
        prompt_text = SYSTEM_PROMPTS.get(é¢„è®¾æç¤ºè¯, é¢„è®¾æç¤ºè¯)
        if è‡ªå®šä¹‰æç¤ºè¯ and è‡ªå®šä¹‰æç¤ºè¯.strip():
            prompt_text = è‡ªå®šä¹‰æç¤ºè¯.strip()
        
        # åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹ç”Ÿæˆå¢å¼ºæç¤ºè¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if extra_options:
            try:
                import qwen3vl_extra_options
                prompt_text = qwen3vl_extra_options.Qwen3VL_ExtraOptions.build_enhanced_prompt(prompt_text, extra_options)
                print(f"[æˆåŠŸ] å·²åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºæç¤ºè¯")
            except (ImportError, AttributeError) as e:
                print(f"[è­¦å‘Š] æ— æ³•å¯¼å…¥Qwen3VLé¢å¤–é€‰é¡¹æ¨¡å— ({e})ï¼Œä½¿ç”¨åŸºç¡€æç¤ºè¯")
        
        # æ„å»ºå¯¹è¯æ¶ˆæ¯
        conversation = [{"role": "user", "content": []}]
            
        # æ·»åŠ å¤šä¸ªå›¾åƒ
        for i, image in enumerate([å›¾åƒ1, å›¾åƒ2, å›¾åƒ3, å›¾åƒ4], 1):
            if image is not None:
                conversation[0]["content"].append({
                    "type": "image",
                    "image": self.image_processor.to_pil(image)
                })
        
        # æ·»åŠ è§†é¢‘ï¼ˆä½œä¸ºå¤šå¸§å›¾åƒåºåˆ—ï¼‰
        if è§†é¢‘ is not None:
            video_frames = [
                Image.fromarray((frame.cpu().numpy() * 255).astype(np.uint8))
                for frame in è§†é¢‘
            ]
            
            # é‡‡æ ·è§†é¢‘å¸§
            if len(video_frames) > è§†é¢‘å¸§æ•°:
                indices = np.linspace(0, len(video_frames) - 1, è§†é¢‘å¸§æ•°, dtype=int)
                sampled_frames = [video_frames[i] for i in indices]
            else:
                sampled_frames = video_frames

            # ç¡®ä¿è‡³å°‘æœ‰2å¸§ï¼ˆQwen3-VL è¦æ±‚ï¼‰
            if sampled_frames and len(sampled_frames) == 1:
                sampled_frames.append(sampled_frames[0])
                
            if sampled_frames:
                conversation[0]["content"].append({
                    "type": "video",
                    "video": sampled_frames
                })

        # æ·»åŠ æ–‡æœ¬æç¤º
        conversation[0]["content"].append({
            "type": "text",
            "text": prompt_text
        })

        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text_prompt = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )
            
        # æå–å›¾åƒå’Œè§†é¢‘ç”¨äºå¤„ç†å™¨
        pil_images = [
            item['image'] for item in conversation[0]['content']
            if item['type'] == 'image'
        ]
        video_frames_list = [
            frame for item in conversation[0]['content']
            if item['type'] == 'video'
            for frame in item['video']
        ]
        videos_arg = [video_frames_list] if video_frames_list else None
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=text_prompt,
            images=pil_images if pil_images else None,
            videos=videos_arg,
            return_tensors="pt"
        )
        
        # å°†è¾“å…¥ç§»åˆ°è®¾å¤‡
        model_inputs = {
            k: v.to(effective_device)
            for k, v in inputs.items()
            if torch.is_tensor(v)
        }

        # è®¾ç½®åœæ­¢æ ‡è®°
        stop_tokens = [self.tokenizer.eos_token_id]
        if hasattr(self.tokenizer, 'eot_id'):
            stop_tokens.append(self.tokenizer.eot_id)

        # ç”Ÿæˆå‚æ•°
        gen_kwargs = {
            "max_new_tokens": æœ€å¤§ä»¤ç‰Œæ•°,
            "num_beams": æŸæœç´¢æ•°é‡,
            "eos_token_id": stop_tokens,
            "pad_token_id": self.tokenizer.pad_token_id
        }

        if æŸæœç´¢æ•°é‡ > 1:
            gen_kwargs["do_sample"] = False
            # å¯¹äºæŸæœç´¢ï¼Œåªæ·»åŠ é‡å¤æƒ©ç½š
            if é‡å¤æƒ©ç½š and é‡å¤æƒ©ç½š > 0 and é‡å¤æƒ©ç½š != 1.0:
                # ç¡®ä¿é‡å¤æƒ©ç½šåœ¨åˆç†èŒƒå›´å†…
                safe_repetition_penalty = max(1.0, min(2.0, é‡å¤æƒ©ç½š))
                gen_kwargs["repetition_penalty"] = safe_repetition_penalty
        else:
            gen_kwargs.update({
                "do_sample": True,
                # ç¡®ä¿æ¸©åº¦åœ¨åˆç†èŒƒå›´å†…
                "temperature": max(0.1, min(1.0, é‡‡æ ·æ¸©åº¦)),
                # ç¡®ä¿top_påœ¨åˆç†èŒƒå›´å†…
                "top_p": max(0.01, min(1.0, æ ¸é‡‡æ ·å‚æ•°))
            })
            # å¯¹äºé‡‡æ ·æ¨¡å¼ï¼Œä¹Ÿå¯ä»¥æ·»åŠ é‡å¤æƒ©ç½šï¼Œä½†è¦ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
            if é‡å¤æƒ©ç½š and é‡å¤æƒ©ç½š > 0 and é‡å¤æƒ©ç½š != 1.0:
                safe_repetition_penalty = max(1.0, min(1.5, é‡å¤æƒ©ç½š))
                gen_kwargs["repetition_penalty"] = safe_repetition_penalty

        # ç”Ÿæˆæ–‡æœ¬
        gen_start = time.time()
        outputs = self.model.generate(**model_inputs, **gen_kwargs)
        gen_time = time.time() - gen_start
        
        input_ids_len = model_inputs["input_ids"].shape[1]
        text = self.tokenizer.decode(
            outputs[0, input_ids_len:],
            skip_special_tokens=True
        )
        
        total_time = time.time() - start_time
        print(f"â±ï¸ è€—æ—¶ç»Ÿè®¡: æ¨¡å‹åŠ è½½ {load_time:.2f}s | æ¨ç†ç”Ÿæˆ {gen_time:.2f}s | æ€»è®¡ {total_time:.2f}s")
        
        if not ä¿æŒæ¨¡å‹åŠ è½½:
            self.clear_model_resources()
        return (text.strip(),)


class Qwen3VL_Chat:
    """Qwen3-VL æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹ - æ”¯æŒå¤šæ¨¡æ€LLMå¯¹è¯"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model_name = None
        self.current_quantization = None
        self.current_device = None
        self.last_seed = -1
        self.device_info = get_device_info()
        self.downloader = ModelDownloader(MODEL_CONFIGS)
        self.image_processor = ImageProcessor()
        print(f"Qwen3VL æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹å·²åˆå§‹åŒ–ã€‚è®¾å¤‡: {self.device_info['device_type']}")
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}")

    def clear_model_resources(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.model is not None:
            print("é‡Šæ”¾æ¨¡å‹èµ„æº...")
            del self.model, self.processor, self.tokenizer
            self.model = self.processor = self.tokenizer = None
            self.current_model_name = self.current_quantization = self.current_device = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def load_model(self, model_name: str, quantization_str: str, device: str = "auto"):
        """åŠ è½½æ¨¡å‹"""
        effective_device = self.device_info["recommended_device"] if device == "auto" else device

        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”é…ç½®ç›¸åŒï¼Œåˆ™è·³è¿‡
        if (self.model is not None and
            self.current_model_name == model_name and
            self.current_quantization == quantization_str and
            self.current_device == effective_device):
            return

        self.clear_model_resources()

        model_info = get_model_info(model_name)

        # æ£€æŸ¥ abliterated æ¨¡å‹çš„è­¦å‘Š
        if model_info.get("abliterated"):
            warning_msg = model_info.get("warning", "æ­¤æ¨¡å‹å·²ç§»é™¤å®‰å…¨è¿‡æ»¤")
            print(f"\n[è­¦å‘Š] è­¦å‘Š: {warning_msg}\n")
        
        # æ£€æŸ¥ FP8 é‡åŒ–æ¨¡å‹çš„ GPU è®¡ç®—èƒ½åŠ›è¦æ±‚
        if model_info.get("quantized"):
            if self.device_info["gpu"]["available"]:
                major, minor = torch.cuda.get_device_capability()
                cc = major + minor / 10
                if cc < 8.9:
                    raise ValueError(
                        f"FP8 æ¨¡å‹éœ€è¦è®¡ç®—èƒ½åŠ› 8.9 æˆ–æ›´é«˜çš„ GPU (ä¾‹å¦‚ RTX 4090)ã€‚"
                        f"æ‚¨çš„ GPU è®¡ç®—èƒ½åŠ›ä¸º {cc}ã€‚è¯·é€‰æ‹©é FP8 æ¨¡å‹ã€‚"
                    )

        model_path = self.downloader.ensure_model_available(model_name)
        adjusted_quantization = check_memory_requirements(model_name, quantization_str, self.device_info)
        
        quant_config, load_dtype = None, torch.float16
        
        # ä»…å¯¹éé¢„é‡åŒ–æ¨¡å‹åº”ç”¨é‡åŒ–é…ç½®
        if not get_model_info(model_name).get("quantized", False):
            if adjusted_quantization == Quantization.Q4_BIT:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
                load_dtype = None
            elif adjusted_quantization == Quantization.Q8_BIT:
                quant_config = BitsAndBytesConfig(load_in_8bit=True)
                load_dtype = None

        device_map = "auto"
        if effective_device == "cuda" and torch.cuda.is_available():
            device_map = {"": 0}

        # æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,
            "attn_implementation": "flash_attention_2" if check_flash_attention() else "sdpa",
            "use_safetensors": True,
            "trust_remote_code": True
        }
        
        if quant_config:
            load_kwargs["quantization_config"] = quant_config

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{model_name}'...")
        # åŠ è½½æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
        self.model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs).eval()
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        self.current_model_name = model_name
        self.current_quantization = quantization_str
        self.current_device = effective_device
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")

    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        model_names = [name for name in MODEL_CONFIGS.keys() if not name.startswith('_')]
        default_model = model_names[4] if len(model_names) > 4 else model_names[0]

        return {
            "required": {
                "æ¨¡å‹é€‰æ‹©": (model_names, {"default": default_model}),
                "é‡åŒ–çº§åˆ«": (list(Quantization.get_values()), {"default": Quantization.NONE}),
                "ç”¨æˆ·è¾“å…¥": ("STRING", {
                    "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                    "multiline": True,
                    "placeholder": "è¾“å…¥ä½ æƒ³è¦å¯¹è¯çš„å†…å®¹"
                }),
                "ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚",
                    "multiline": True,
                    "placeholder": "å®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼"
                }),
                "æ¸©åº¦": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 1.0, "step": 0.1}),
                "Top-P": ("FLOAT", {"default": 0.90, "min": 0.0, "max": 1.0, "step": 0.01}),
                "æœ€å¤§é•¿åº¦": ("INT", {"default": 2048, "min": 64, "max": 4096, "step": 16}),
                "éšæœºç§å­": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "tooltip": "éšæœºç§å­ï¼Œ-1ä¸ºéšæœº"
                }),
                "ç§å­æ§åˆ¶": (["éšæœº", "å›ºå®š", "é€’å¢"], {"default": "éšæœº"}),
                "å¼€å¯TF32åŠ é€Ÿ": ("BOOLEAN", {"default": True, "tooltip": "å¯ç”¨TF32åŠ é€Ÿï¼ˆä»…æ”¯æŒAmpereåŠä»¥ä¸Šæ¶æ„æ˜¾å¡ï¼Œå¦‚30/40/50ç³»ï¼Œèƒ½æ˜¾è‘—æå‡é€Ÿåº¦ï¼‰"}),
                "ä¿æŒæ¨¡å‹åŠ è½½": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "å›¾åƒ1": ("IMAGE",),
                "å›¾åƒ2": ("IMAGE",),
                "å›¾åƒ3": ("IMAGE",),
                "å›¾åƒ4": ("IMAGE",),
                "Qwen3VLé¢å¤–é€‰é¡¹": ("QWEN3VL_EXTRA_OPTIONS", {
                    "tooltip": "å¯é€‰çš„Qwen3VLé¢å¤–é€‰é¡¹ï¼Œè¿æ¥Qwen3VLé¢å¤–é€‰é¡¹èŠ‚ç‚¹"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("AIå›å¤",)
    FUNCTION = "chat"
    CATEGORY = "NakuNode-QWen3VL"

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        seed_control = kwargs.get("ç§å­æ§åˆ¶", "éšæœº")
        seed = kwargs.get("éšæœºç§å­", -1)

        # éšæœºå’Œé€’å¢æ¨¡å¼ä¸‹ï¼Œå¼ºåˆ¶æ›´æ–° (è¿”å› NaN)
        if seed_control in ["éšæœº", "é€’å¢"]:
            return float("nan")

        # å›ºå®šæ¨¡å¼ä¸‹ï¼Œä»…å½“ç§å­å€¼å˜åŒ–æ—¶æ›´æ–°
        return seed

    @torch.no_grad()
    def chat(self, **kwargs):
        """æ™ºèƒ½å¯¹è¯å¤„ç†å‡½æ•°"""
        # æå–å‚æ•°
        æ¨¡å‹åç§° = kwargs.get("æ¨¡å‹é€‰æ‹©")
        é‡åŒ–çº§åˆ« = kwargs.get("é‡åŒ–çº§åˆ«")
        ç”¨æˆ·è¾“å…¥ = kwargs.get("ç”¨æˆ·è¾“å…¥")
        ç³»ç»Ÿè§’è‰²å®šä¹‰ = kwargs.get("ç³»ç»Ÿè§’è‰²å®šä¹‰")
        æ¸©åº¦ = kwargs.get("æ¸©åº¦")
        æœ€å¤§é•¿åº¦ = kwargs.get("æœ€å¤§é•¿åº¦")
        éšæœºç§å­ = kwargs.get("éšæœºç§å­")
        ç§å­æ§åˆ¶ = kwargs.get("ç§å­æ§åˆ¶")
        ä¿æŒæ¨¡å‹åŠ è½½ = kwargs.get("ä¿æŒæ¨¡å‹åŠ è½½")
        å¼€å¯TF32åŠ é€Ÿ = kwargs.get("å¼€å¯TF32åŠ é€Ÿ", False)
        å›¾åƒ1 = kwargs.get("å›¾åƒ1")
        å›¾åƒ2 = kwargs.get("å›¾åƒ2")
        å›¾åƒ3 = kwargs.get("å›¾åƒ3")
        å›¾åƒ4 = kwargs.get("å›¾åƒ4")
        extra_options = kwargs.get("Qwen3VLé¢å¤–é€‰é¡¹", None)
        # å¤„ç†Top-På‚æ•°ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
        top_p = kwargs.get("Top-P", 0.90)

        start_time = time.time()

        # è®¾ç½® TF32 åŠ é€Ÿ
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = å¼€å¯TF32åŠ é€Ÿ
            torch.backends.cudnn.allow_tf32 = å¼€å¯TF32åŠ é€Ÿ
            if å¼€å¯TF32åŠ é€Ÿ:
                print("[åŠ é€Ÿ] å·²å¼€å¯ TF32 åŠ é€Ÿæ¨¡å¼")
        
        # ç§å­é€»è¾‘å¤„ç†
        if ç§å­æ§åˆ¶ == "å›ºå®š":
            effective_seed = éšæœºç§å­ if éšæœºç§å­ != -1 else random.randint(0, 2147483647)
        elif ç§å­æ§åˆ¶ == "éšæœº":
            effective_seed = random.randint(0, 2147483647)
        elif ç§å­æ§åˆ¶ == "é€’å¢":
            if self.last_seed == -1:
                effective_seed = éšæœºç§å­ if éšæœºç§å­ != -1 else random.randint(0, 2147483647)
            else:
                effective_seed = self.last_seed + 1
        else:
            effective_seed = random.randint(0, 2147483647)
        
        self.last_seed = effective_seed
        print(f"ä½¿ç”¨éšæœºç§å­: {effective_seed} (æ¨¡å¼: {ç§å­æ§åˆ¶})")
        torch.manual_seed(effective_seed)
        
        # æ£€æŸ¥ transformers ç‰ˆæœ¬
        if version.parse(transformers.__version__) < version.parse("4.57.0"):
            raise RuntimeError(f"transformers ç‰ˆæœ¬è¿‡ä½: å½“å‰ç‰ˆæœ¬ {transformers.__version__}, éœ€è¦ >= 4.57.0")

        load_start = time.time()
        self.load_model(æ¨¡å‹åç§°, é‡åŒ–çº§åˆ«, "auto")
        load_time = time.time() - load_start
        effective_device = self.current_device
        
        # å¤„ç†ç³»ç»Ÿè§’è‰²å®šä¹‰ï¼Œåº”ç”¨é¢å¤–é€‰é¡¹
        system_prompt = ç³»ç»Ÿè§’è‰²å®šä¹‰.strip() if ç³»ç»Ÿè§’è‰²å®šä¹‰ else ""
        
        # åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if extra_options and system_prompt:
            try:
                import qwen3vl_extra_options
                system_prompt = qwen3vl_extra_options.Qwen3VL_ExtraOptions.build_enhanced_prompt(system_prompt, extra_options)
                print(f"[æˆåŠŸ] å·²åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºç³»ç»Ÿè§’è‰²")
            except (ImportError, AttributeError) as e:
                print(f"[è­¦å‘Š] æ— æ³•å¯¼å…¥Qwen3VLé¢å¤–é€‰é¡¹æ¨¡å— ({e})ï¼Œä½¿ç”¨åŸºç¡€ç³»ç»Ÿè§’è‰²")
        
        # æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼Œå…ˆæ·»åŠ ç³»ç»Ÿè§’è‰²å®šä¹‰
        conversation = []
            
        # æ·»åŠ ç³»ç»Ÿè§’è‰²å®šä¹‰ï¼ˆå¦‚æœæä¾›ï¼‰
        if system_prompt:
            conversation.append({
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            })
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_content = []
        
        # æ·»åŠ å¤šä¸ªå›¾åƒ
        for i, image in enumerate([å›¾åƒ1, å›¾åƒ2, å›¾åƒ3, å›¾åƒ4], 1):
            if image is not None:
                user_content.append({
                    "type": "image",
                    "image": self.image_processor.to_pil(image)
                })
        
        # æ·»åŠ ç”¨æˆ·æ–‡æœ¬è¾“å…¥
        user_content.append({
            "type": "text",
            "text": ç”¨æˆ·è¾“å…¥
        })
        
        conversation.append({
            "role": "user",
            "content": user_content
        })

        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text_prompt = self.processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # æå–å›¾åƒç”¨äºå¤„ç†å™¨
        pil_images = []
        for msg in conversation:
            if msg['role'] == 'user':
                pil_images.extend([
                    item['image'] for item in msg['content']
                    if item['type'] == 'image'
                ])
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=text_prompt,
            images=pil_images if pil_images else None,
            return_tensors="pt"
        )
        
        # å°†è¾“å…¥ç§»åˆ°è®¾å¤‡
        model_inputs = {
            k: v.to(effective_device)
            for k, v in inputs.items()
            if torch.is_tensor(v)
        }

        # è®¾ç½®åœæ­¢æ ‡è®°
        stop_tokens = [self.tokenizer.eos_token_id]
        if hasattr(self.tokenizer, 'eot_id'):
            stop_tokens.append(self.tokenizer.eot_id)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªè¯†åˆ«çš„å‚æ•°
        remaining_kwargs = {k: v for k, v in kwargs.items() if not k.startswith(('æ¨¡å‹é€‰æ‹©', 'é‡åŒ–çº§åˆ«', 'ç”¨æˆ·è¾“å…¥', 'ç³»ç»Ÿè§’è‰²å®šä¹‰', 'æ¸©åº¦', 'Top-P', 'æœ€å¤§é•¿åº¦', 'éšæœºç§å­', 'ç§å­æ§åˆ¶', 'ä¿æŒæ¨¡å‹åŠ è½½', 'å›¾åƒ', 'å¼€å¯TF32åŠ é€Ÿ'))}
        if remaining_kwargs:
            print(f"[Qwen3VL_Chat] æœªè¯†åˆ«çš„å‚æ•°å·²å¿½ç•¥: {', '.join(remaining_kwargs.keys())}")

        # ç”Ÿæˆå‚æ•°
        gen_kwargs = {
            "max_new_tokens": æœ€å¤§é•¿åº¦,
            "do_sample": True,
            # ç¡®ä¿æ¸©åº¦åœ¨åˆç†èŒƒå›´å†…
            "temperature": max(0.1, min(1.0, æ¸©åº¦)),
            # ç¡®ä¿top_påœ¨åˆç†èŒƒå›´å†…
            "top_p": max(0.01, min(1.0, top_p)),
            "eos_token_id": stop_tokens,
            "pad_token_id": self.tokenizer.pad_token_id
        }

        # ç”Ÿæˆæ–‡æœ¬
        gen_start = time.time()
        outputs = self.model.generate(**model_inputs, **gen_kwargs)
        gen_time = time.time() - gen_start
        
        input_ids_len = model_inputs["input_ids"].shape[1]
        text = self.tokenizer.decode(
            outputs[0, input_ids_len:],
            skip_special_tokens=True
        )
        
        total_time = time.time() - start_time
        print(f"â±ï¸ è€—æ—¶ç»Ÿè®¡: æ¨¡å‹åŠ è½½ {load_time:.2f}s | æ¨ç†ç”Ÿæˆ {gen_time:.2f}s | æ€»è®¡ {total_time:.2f}s")
        
        if not ä¿æŒæ¨¡å‹åŠ è½½:
            self.clear_model_resources()
        return (text.strip(),)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "Qwen3VL_Advanced": Qwen3VL_Advanced,
    "Qwen3VL_Chat": Qwen3VL_Chat,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VL_Advanced": "NakuNode-QWen3VL",
    "Qwen3VL_Chat": "NakuNode-QWen3VLæ™ºèƒ½å¯¹è¯",
}
